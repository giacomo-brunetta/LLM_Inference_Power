{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fefe612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbrun/miniforge3/envs/vllm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-06 19:10:39 [__init__.py:207] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 19:10:39,359\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bfdcba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10103 for 10,000 actual\n",
    "\n",
    "\n",
    "ds = load_dataset(\"lmsys/lmsys-chat-1m\", split=f\"train[0:1015]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d689128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_len(chat):\n",
    "    return sum([len(msg['content']) for msg in chat])\n",
    "\n",
    "inputs = []\n",
    "sampling_params = []\n",
    "\n",
    "input_lengths = []\n",
    "output_lengths = []\n",
    "\n",
    "for chat in ds['conversation']:\n",
    "    input_chat = chat[:-1]\n",
    "    output_chat = chat[-1]\n",
    "\n",
    "    in_len = chat_len(input_chat)\n",
    "    out_len = chat_len([output_chat])\n",
    "\n",
    "    #skip samples that are too long\n",
    "    if in_len > 10000 or out_len > 10000:\n",
    "        continue\n",
    "\n",
    "    inputs.append(input_chat)\n",
    "    input_lengths.append(in_len)\n",
    "    output_lengths.append(out_len)\n",
    "\n",
    "    sampling_params.append(SamplingParams(\n",
    "                n=1,\n",
    "                temperature= 1.0,\n",
    "                top_p=1.0,\n",
    "                ignore_eos=True,\n",
    "                max_tokens= max(output_lengths[-1]//4,64),\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "195b5c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d21247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_166918/4102317643.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.scatter(np.log(input_lengths), np.log(output_lengths), s =1 )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0xfffe22515400>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_lengths = np.array(input_lengths)\n",
    "output_lengths = np.array(output_lengths)\n",
    "\n",
    "plt.scatter(np.log(input_lengths), np.log(output_lengths), s =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e59cc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Qwen/Qwen2.5-14B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56923c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-06 19:10:48 [config.py:575] This model supports multiple tasks: {'score', 'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 05-06 19:10:48 [config.py:1485] Defaulting to use mp for distributed inference\n",
      "INFO 05-06 19:10:48 [config.py:1660] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 05-06 19:10:48 [llm_engine.py:235] Initializing a V0 LLM engine (v0.7.4.dev172+gd54990da.d20250302) with config: model='Qwen/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "WARNING 05-06 19:10:49 [multiproc_worker_utils.py:309] Reducing Torch parallelism from 80 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 05-06 19:10:49 [custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167186)\u001b[0;0m INFO 05-06 19:10:49 [multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 05-06 19:10:50 [cuda.py:268] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167186)\u001b[0;0m INFO 05-06 19:10:50 [cuda.py:268] Using Flash Attention backend.\n",
      "INFO 05-06 19:10:51 [utils.py:939] Found nccl from library libnccl.so.2\n",
      "ERROR 05-06 19:10:51 [pynccl_wrapper.py:229] Failed to load NCCL library from libnccl.so.2. It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise, the nccl library might not exist, be corrupted or it does not support the current platform Linux-5.15.0-105-generic-aarch64-with-glibc2.35. If you already have the library, please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167186)\u001b[0;0m INFO 05-06 19:10:51 [utils.py:939] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167186)\u001b[0;0m INFO 05-06 19:10:51 [parallel_state.py:948] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "ERROR 05-06 19:10:51 [pynccl_wrapper.py:229] Failed to load NCCL library from libnccl.so.2. It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise, the nccl library might not exist, be corrupted or it does not support the current platform Linux-5.15.0-105-generic-aarch64-with-glibc2.35. If you already have the library, please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167186)\u001b[0;0m INFO 05-06 19:10:51 [parallel_state.py:948] rank 1 in world size 2 is assigned as DP rank 0, PP rank 1, TP rank 0\n",
      "INFO 05-06 19:10:51 [model_runner.py:1110] Starting to load model Qwen/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167186)\u001b[0;0m INFO 05-06 19:10:51 [model_runner.py:1110] Starting to load model Qwen/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167186)\u001b[0;0m INFO 05-06 19:10:51 [weight_utils.py:257] Using model weights format ['*.safetensors']\n",
      "INFO 05-06 19:10:51 [weight_utils.py:257] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:00<00:00,  7.26it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:01<00:01,  3.47it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:01<00:00,  4.30it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:01<00:00,  3.03it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:02<00:00,  2.33it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:02<00:00,  2.96it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:02<00:00,  3.26it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=167186)\u001b[0;0m INFO 05-06 19:10:54 [model_runner.py:1117] Loading model weights took 13.7879 GB and 2.865004 seconds\n",
      "INFO 05-06 19:10:54 [model_runner.py:1117] Loading model weights took 13.7879 GB and 2.778730 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167186)\u001b[0;0m INFO 05-06 19:10:55 [worker.py:267] Memory profiling takes 0.63 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167186)\u001b[0;0m INFO 05-06 19:10:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB\n",
      "INFO 05-06 19:10:55 [worker.py:267] Memory profiling takes 0.68 seconds\n",
      "INFO 05-06 19:10:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB\n",
      "INFO 05-06 19:10:55 [worker.py:267] model weights take 13.79GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 21.32GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167186)\u001b[0;0m INFO 05-06 19:10:55 [executor_base.py:111] # cuda blocks: 13732, # CPU blocks: 2730\n",
      "INFO 05-06 19:10:55 [executor_base.py:116] Maximum concurrency for 32768 tokens per request: 6.71x\n",
      "INFO 05-06 19:10:55 [worker.py:267] model weights take 13.79GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 20.12GiB.\n",
      "INFO 05-06 19:10:58 [llm_engine.py:441] init engine (profile, create kv cache, warmup model) took 4.08 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "llm = LLM(\n",
    "        model=model_name,\n",
    "        speculative_model=None,\n",
    "        num_speculative_tokens=None,\n",
    "        speculative_draft_tensor_parallel_size=None,\n",
    "        tokenizer=None,\n",
    "        quantization=None,\n",
    "        tensor_parallel_size=1,\n",
    "        pipeline_parallel_size=2,\n",
    "        trust_remote_code=True,\n",
    "        dtype='bfloat16',\n",
    "        # max_model_len=4096 ,\n",
    "        enforce_eager=True,\n",
    "        kv_cache_dtype='auto',\n",
    "        device='cuda',\n",
    "        block_size=16,\n",
    "        enable_chunked_prefill=True,\n",
    "        gpu_memory_utilization=0.90,\n",
    "        load_format='auto',\n",
    "        distributed_executor_backend=None,\n",
    "        enable_prefix_caching=True,\n",
    "        disable_sliding_window=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b777587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-06 19:11:19 [chat_utils.py:825] Skipping multimodal part (type: 'text') with empty / unparsable content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Pipeline parallelism is only supported through AsyncLLMEngine as performance will be severely degraded otherwise.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# warm_up\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/entrypoints/llm.py:749\u001b[39m, in \u001b[36mLLM.chat\u001b[39m\u001b[34m(self, messages, sampling_params, use_tqdm, lora_request, chat_template, chat_template_content_format, add_generation_prompt, continue_final_message, tools, mm_processor_kwargs)\u001b[39m\n\u001b[32m    745\u001b[39m         prompt[\u001b[33m\"\u001b[39m\u001b[33mmm_processor_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = mm_processor_kwargs\n\u001b[32m    747\u001b[39m     prompts.append(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/utils.py:1080\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1073\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1075\u001b[39m         warnings.warn(\n\u001b[32m   1076\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1077\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1078\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1080\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/entrypoints/llm.py:470\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[39m\n\u001b[32m    460\u001b[39m     sampling_params = \u001b[38;5;28mself\u001b[39m.get_default_sampling_params()\n\u001b[32m    462\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    463\u001b[39m     prompts=parsed_prompts,\n\u001b[32m    464\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    467\u001b[39m     guided_options=guided_options_request,\n\u001b[32m    468\u001b[39m     priority=priority)\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/entrypoints/llm.py:1377\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1375\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1379\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/engine/llm_engine.py:1319\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1268\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs one decoding iteration and returns newly generated results.\u001b[39;00m\n\u001b[32m   1269\u001b[39m \n\u001b[32m   1270\u001b[39m \u001b[33;03m.. figure:: https://i.imgur.com/sv2HssD.png\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1316\u001b[39m \u001b[33;03m    >>>         break\u001b[39;00m\n\u001b[32m   1317\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parallel_config.pipeline_parallel_size > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1320\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPipeline parallelism is only supported through AsyncLLMEngine \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas performance will be severely degraded otherwise.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1323\u001b[39m \u001b[38;5;66;03m# For llm_engine, there is no pipeline parallel support, so the engine\u001b[39;00m\n\u001b[32m   1324\u001b[39m \u001b[38;5;66;03m# used is always 0.\u001b[39;00m\n\u001b[32m   1325\u001b[39m virtual_engine = \u001b[32m0\u001b[39m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Pipeline parallelism is only supported through AsyncLLMEngine as performance will be severely degraded otherwise."
     ]
    }
   ],
   "source": [
    "# warm_up\n",
    "\n",
    "results = llm.chat(\n",
    "    inputs[0:50],\n",
    "    sampling_params=sampling_params[0:50],\n",
    "    use_tqdm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a725905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from power_utils import gpuPowerProbe\n",
    "\n",
    "gpus = 2\n",
    "interval = 1\n",
    "\n",
    "inference_powers = []\n",
    "inference_powers_time = []\n",
    "power_avgs = []\n",
    "power_peaks = []\n",
    "energies = []\n",
    "power_profiles = []\n",
    "\n",
    "for id in range(gpus):\n",
    "    power_profiles.append(gpuPowerProbe(interval=interval, gpu_id=id))\n",
    "    power_profiles[id].start()\n",
    "\n",
    "results = llm.chat(\n",
    "    inputs,\n",
    "    sampling_params=sampling_params,\n",
    "    use_tqdm=True\n",
    ")\n",
    "\n",
    "for power_profile in power_profiles:\n",
    "    power, times = power_profile.stop()\n",
    "    inference_powers.append(power)\n",
    "    inference_powers_time.append(times)\n",
    "    power_profile.destroy()\n",
    "\n",
    "print(\"\\n----------------Power-----------------------\")\n",
    "for id in range(gpus):\n",
    "    print(f\"GPU {id}:\")\n",
    "    power = np.array(inference_powers[id]) / 1000  # to Watt\n",
    "    times = np.array(inference_powers_time[id])\n",
    "    avg_power = np.mean(power)\n",
    "    peak_power = np.max(power)\n",
    "    energy = np.sum(power*times)\n",
    "\n",
    "    power_avgs.append(avg_power)\n",
    "    power_peaks.append(peak_power)\n",
    "    energies.append(energy)\n",
    "\n",
    "    print(f\"    Power avg : {avg_power :.3f} W\")\n",
    "    print(f\"    Power peak: {peak_power :.3f} W\")\n",
    "    print(f\"    Energy    : {energy :.3f} J\")\n",
    "    plt.figsize(4,12)\n",
    "    plt.plot(np.cumsum(inference_powers_time[id]), power, label=f'GPU {id}')\n",
    "\n",
    "    plt.title(\"Power consumption\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(f'Time ({interval} sec intervals)')\n",
    "    plt.ylabel('Power Consumption (W)')\n",
    "    plt.savefig('gpu_power_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11187c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttfts = []\n",
    "latencies = []\n",
    "latencies_with_queue = []\n",
    "decode_times = []\n",
    "queue_times = []\n",
    "\n",
    "input_tokens = []\n",
    "output_tokens = []\n",
    "\n",
    "start = results[0].metrics.first_scheduled_time\n",
    "end = 0\n",
    "\n",
    "for resp in results:\n",
    "    end = max(end, resp.metrics.finished_time)\n",
    "    # Time to first token (TTFT):\n",
    "    ttfts.append(resp.metrics.first_token_time - resp.metrics.first_scheduled_time)\n",
    "    latencies.append(resp.metrics.finished_time - resp.metrics.first_scheduled_time)\n",
    "    # Total end-to-end latency:\n",
    "    latencies_with_queue.append(resp.metrics.finished_time - resp.metrics.arrival_time)\n",
    "    # Decode duration (after first token):\n",
    "    decode_times.append(resp.metrics.last_token_time - resp.metrics.first_token_time)\n",
    "    # Queue wait time:\n",
    "    queue_times.append(resp.metrics.time_in_queue)\n",
    "\n",
    "    input_tokens.append(len(resp.prompt_token_ids))\n",
    "    output_tokens.append(len(resp.outputs[0].token_ids))\n",
    "\n",
    "latency_overall = end - start\n",
    "\n",
    "tp_overall = (np.sum(input_tokens) + np.sum(output_tokens)) / latency_overall\n",
    "out_tp_overall = np.sum(output_tokens) / latency_overall\n",
    "in_tp_overall = np.sum(input_tokens)  / latency_overall\n",
    "\n",
    "ttfts = np.array(ttfts)\n",
    "latencies = np.array(latencies)\n",
    "latencies_with_queue = np.array(latencies_with_queue)\n",
    "decode_times = np.array(decode_times)\n",
    "queue_times = np.array(queue_times)\n",
    "\n",
    "input_tokens = np.array(input_tokens)\n",
    "output_tokens = np.array(output_tokens)\n",
    "\n",
    "throughputs = (input_tokens + output_tokens) / decode_times\n",
    "out_throughputs = (output_tokens) / decode_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc78b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tp_overall, out_tp_overall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
